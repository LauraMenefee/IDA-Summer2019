{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExercise 12 Princesses\\nText analysis\\nAug 13,2019\\n\\nLaura Menefee\\n\\nFrom Canvas.....\\n\\n********\\nFrom this link (Links to an external site.), download the 12dancingprincesses.txt file. \\nThen read the file and use the NLTK library to tokenize each word in the text. \\nBelow is some code to help you get started with loading the file. \\nThe text in square brackets [] is where you would add your own code.\\n....\\n\\n**********\\n\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 12 Princesses\n",
    "Text Analysis\n",
    "Aug 13,2019\n",
    "\n",
    "Laura Menefee\n",
    "\n",
    "From Canvas.....\n",
    "\n",
    "********\n",
    "From this link (Links to an external site.), download the 12dancingprincesses.txt file. \n",
    "Then read the file and use the NLTK library to tokenize each word in the text. \n",
    "Below is some code to help you get started with loading the file. \n",
    "The text in square brackets [] is where you would add your own code.\n",
    "....\n",
    "\n",
    "After using that code to load and tokenizing each word, then remove the punctuation and filler \n",
    "words (stopwords) from the list of tokens. Lastly, get the top 10 words from the text.\n",
    "**********\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from string import punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load 12dancingprincesses.txt file \n",
    "filepath = \"datasets/12dancingprincesses.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in file and process\n",
    "newwordsls = []   ## new empty list to hold the tokens at the end\n",
    "\n",
    "with open(filepath, 'r'  ) as princess:\n",
    "    \n",
    "    for line in princess:\n",
    "        cline = line.strip() #get rid of new line character\n",
    "        \n",
    "        if cline =='': \n",
    "            pass     #this will skip over lines that only had a newline and are now blank\n",
    "        else:\n",
    "            tknls = word_tokenize(cline)\n",
    "        \n",
    "        for token in tknls:\n",
    "            newwordsls.append(token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(newwordsls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(newwordsls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newwordsls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newwordsls[0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newwordsls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Need to clean up and tokenize each word in list\n",
    "# set all words to lower case\n",
    "lowerwordsls = []\n",
    "\n",
    "for token in newwordsls:\n",
    "    token_low = token.lower()\n",
    "    lowerwordsls.append(token_low)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowerwordsls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set lowerwordsls to new list, to clean out puncuation\n",
    "lowerwordswopuncls = lowerwordsls\n",
    "\n",
    "str = \"’, ‘ \"\n",
    "punctuation = punctuation + str\n",
    "\n",
    "for token in lowerwordsls:\n",
    "    if token in punctuation:\n",
    "        lowerwordswopuncls.remove(token)\n",
    "    else:\n",
    "        if token in [\"--\", \"--\",\";\",\"'\", \"`\"]:  ##  for some reason had to add '--' to get second one removed\n",
    "            lowerwordswopuncls.remove(token)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowerwordswopuncls.sort()\n",
    "    \n",
    "lowerwordswopuncls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lowerwordswopuncls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #  take out stop words \n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  take out stop words \n",
    "\n",
    "rm_count = 0\n",
    "new_words = []\n",
    "\n",
    "for token in lowerwordswopuncls:\n",
    "    if token not in eng_stopwords:\n",
    "        new_words.append(token)\n",
    "    else: rm_count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## what are the top 10 words?\n",
    "fd_nw = FreqDist(new_words)\n",
    "fd_nw.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
